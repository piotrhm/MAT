# MAT-D
The implementation of the Molecule Attention Transformer with Decoder.

## Code
- `EXAMPLE.ipynb` jupytest notebook with smoke runs and training examples including gumbel softmax.

## Pretrained weights
TBA

## Results
TBA


## Acknowledgments
Transformer implementation is inspired by [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html).
